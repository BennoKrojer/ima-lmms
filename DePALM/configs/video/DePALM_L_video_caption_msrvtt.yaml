image_res: 224
batch_size_train: 2

batch_size_test: 16



optimizer: {opt: adamW, lr: 2e-4, weight_decay: 0.02, prompt_lr: 1e-3}
schedular: {sched: cosine, scheduler_groups: 0 , lr: 2e-4, epochs: 20, min_lr: 1e-5, decay_rate: 1, warmup_lr: 1e-4, warmup_epochs: 4, cooldown_epochs: 0}

use_vis_prefix: True
# start_layer_idx: 19
# end_layer_idx: 31
start_layer_idx: 0
end_layer_idx: 1



injected_hidden_states: 1 # max 2
shared_connector: True

lm_loss_weight: 0.1

unfreeze_text_layer_norm: False
unfreeze_vision_layer_norm: False


num_workers: 4



replace_added_tokens: True


use_cache: False

shift_labels: False

append_eos_token: True

num_beams: 3
do_sample: False



# Prompt tuning
# prompt_tuning: True
prompt_len: 10
prompt_lr: 1e-3
mlp: False

batch_size_test: 8


interaction_type: cat


target: prompt
residual: True
use_attention_film: True
interaction_config: {'frozen': False} # qsformer


# connector_type: cnn
connector_type: trans
multihead_connector: True

## token pruning
vis_tokens: all



connector_config: {'hidden_dim': 256, 'num_heads': 8, 'num_layers': 5, 'output_length': 10, 'activation': 'relu', 'qs_former': True} # qsformer


inject_outside: True



dataset_name: 'msrvtt'

train_split: 'msrvtt_caption_train7k'
val_split: 'msrvtt_caption_test'
test_split: 'msrvtt_caption_test'


num_frames: 8
sample_type: 'fps1'
num_tries: 1
image_size: 224


pretrained_model: '.cache/torch/hub/checkpoints/TimeSformer_divST_8x32_224_K600.pyth'
