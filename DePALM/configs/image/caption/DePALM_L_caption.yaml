image_res: 224
batch_size_train: 2



warm_up: True



optimizer: {opt: adamW, lr: 2e-4, weight_decay: 0.02, prompt_lr: 1e-3}
schedular: {sched: cosine, scheduler_groups: 0 , lr: 2e-4, epochs: 20, min_lr: 1e-5, decay_rate: 1, warmup_lr: 1e-4, warmup_epochs: 4, cooldown_epochs: 0}

use_vis_prefix: True
# start_layer_idx: 19
# end_layer_idx: 31
start_layer_idx: 0
end_layer_idx: 1
# start_layer_idx: 1
# end_layer_idx: 31
# interaction_step: 4
# start_layer_idx: 0
# end_layer_idx: 31
# select_higher_step: True
# interaction_step: 4


injected_hidden_states: 1
shared_connector: True

lm_loss_weight: 0.1

unfreeze_text_layer_norm: False
unfreeze_vision_layer_norm: False


num_workers: 4



replace_added_tokens: True


use_cache: False

shift_labels: False

append_eos_token: True

num_beams: 3
do_sample: False



# Prompt tuning
# prompt_tuning: True
prompt_len: 10
prompt_lr: 1e-3
mlp: False

batch_size_test: 16

## save hidden states
# valid_topk: 40
# batch_size_test: 1
# save_hidden_states: True
# num_beams: 1

interaction_type: cat
# interaction_type: qsformer

# interaction_type: ['cat', 'ca']
# target: ['prompt', 'all']

target: prompt
residual: True
use_attention_film: True
interaction_config: {'frozen': False} # qsformer


# connector_type: cnn
connector_type: trans
multihead_connector: True

## token pruning
vis_tokens: all

connector_config: {'hidden_dim': 256, 'num_heads': 8, 'num_layers': 5, 'output_length': 10, 'activation': 'relu', 'qs_former': True} # qsformer


inject_outside: True

# train_topk: 0.1
